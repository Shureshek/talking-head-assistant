import asyncio
import queue
from queue import Queue, Empty
import threading
import sounddevice as sd
import numpy as np
import soundfile as sf
import subprocess
from pathlib import Path
import cv2
from datetime import datetime
import torch
import webrtcvad
from collections import deque
from pynput import keyboard as pynput_keyboard  # –ó–∞–º–µ–Ω–∞ –Ω–∞ pynput –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ key listening
from src.asr.whisper_asr import WhisperTranscriber
from src.voice_clone.clone_manager import VoiceCloneManager
from qwen_tts import Qwen3TTSModel
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
SAMPLE_RATE = 16000
CHUNK_SIZE = 480  # 30ms –¥–ª—è VAD (16000 * 0.03)
VAD_AGGRESSIVENESS = 2  # 0-3, –≥–¥–µ 3 —Å–∞–º—ã–π –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π
SILENCE_TIMEOUT = 1.5  # —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ—Ä–∞–∑—ã
MIN_SPEECH_DURATION = 0.5  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
USE_PUSH_TO_TALK = False
PUSH_TO_TALK_KEY = pynput_keyboard.Key.space  # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º pynput Key
push_to_talk_active = threading.Event()
audio_data_queue = Queue()  # thread-safe –æ—á–µ—Ä–µ–¥—å –∏–∑ stdlib
stop_event = threading.Event()

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
conversation_history = [
    {"role": "system", "content": "–¢—ã –∫–æ–∫–µ—Ç–ª–∏–≤–∞—è —Ñ–ª–∏—Ä—Ç—É—é—â–∞—è –¥–µ–≤—É—à–∫–∞ –ø–æ –∏–º–µ–Ω–∏ –Æ–ª—è. –ì–æ–≤–æ—Ä–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."}
]
MAX_HISTORY_LENGTH = 20  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –ø–∞–º—è—Ç–∏

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TTS –º–æ–¥–µ–ª–∏
tts_model = Qwen3TTSModel.from_pretrained(
    "Qwen/Qwen3-TTS-12Hz-0.6B-Base",
    device_map="cuda:0",
    dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM (Qwen1.5-7B-Chat –ª–æ–∫–∞–ª—å–Ω–æ)
model_id = "Qwen/Qwen1.5-7B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cuda:0",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)

"""–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞"""
transcriber = WhisperTranscriber()
manager = VoiceCloneManager(model=tts_model)
person_name = "Julia"
print("üé≠ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
prompt_items = manager.load_or_create_clone(person_name)
print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")

# ==================== VoiceActivityDetector ====================
class VoiceActivityDetector:
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)
        self.chunk_size = int(sample_rate * 0.03)
        self.silence_frames_threshold = int(SILENCE_TIMEOUT / 0.03)
        self.silence_counter = 0
        self.is_speaking = False
        self.audio_buffer = []

        # –î–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤ –∏ edge-detection PTT
        self.last_use_ptt = False          # –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∂–∏–º (VAD / PTT)
        self.last_ptt_state = False        # –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–Ω–æ–ø–∫–∏ PTT

    def process_chunk(self, audio_chunk):
        """VAD-—Ä–µ–∂–∏–º ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —Ñ—Ä–∞–∑–∞ –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å –ø–æ —Ç–∏—à–∏–Ω–µ"""
        try:
            if len(audio_chunk) != self.chunk_size:
                if len(audio_chunk) < self.chunk_size:
                    padding = np.zeros(self.chunk_size - len(audio_chunk), dtype=np.int16)
                    audio_chunk = np.concatenate([audio_chunk, padding])
                else:
                    audio_chunk = audio_chunk[:self.chunk_size]

            audio_bytes = audio_chunk.tobytes()
            is_speech = self.vad.is_speech(audio_bytes, self.sample_rate)

            if is_speech:
                self.silence_counter = 0
                if not self.is_speaking:
                    self.is_speaking = True
                    print("üé§ –†–µ—á—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞—é –∑–∞–ø–∏—Å—å...")
            else:
                if self.is_speaking:
                    self.silence_counter += 1
                    if self.silence_counter >= self.silence_frames_threshold:
                        self.is_speaking = False
                        print("‚è∏Ô∏è –†–µ—á—å –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å (VAD), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...")
                        return True

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±—É—Ñ–µ—Ä
            audio_float = audio_chunk.astype(np.float32) / 32768.0
            self.audio_buffer.extend(audio_float)
            return False

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ VAD: {e}")
            return False

    def get_audio(self):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–∞–µ—Ç –±—É—Ñ–µ—Ä"""
        if not self.audio_buffer:
            return np.array([], dtype=np.float32)
        audio = np.array(self.audio_buffer, dtype=np.float32)
        self.audio_buffer.clear()
        self.silence_counter = 0
        self.is_speaking = False
        return audio

# ==================== record_audio_with_vad ====================
async def record_audio_with_vad():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å —Å VAD + PTT. –¢–µ–ø–µ—Ä—å –∫–ª–∞–¥—ë–º –≥–æ—Ç–æ–≤–æ–µ –∞—É–¥–∏–æ –≤ –æ—á–µ—Ä–µ–¥—å."""
    vad_detector = VoiceActivityDetector()
    stop_recording = threading.Event()

    def audio_callback(indata, frames, time, status):
        if status:
            print(f"–ê—É–¥–∏–æ —Å—Ç–∞—Ç—É—Å: {status}")

        audio_chunk = np.frombuffer(indata, dtype=np.int16)
        phrase_completed = False

        current_use_ptt = USE_PUSH_TO_TALK
        current_ptt = push_to_talk_active.is_set()

        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ–Ω—ã —Ä–µ–∂–∏–º–∞
        if current_use_ptt != vad_detector.last_use_ptt:
            print(f"üéõ –†–µ–∂–∏–º –∏–∑–º–µ–Ω—ë–Ω ‚Üí {'Push-to-talk' if current_use_ptt else 'VAD'}")
            if current_use_ptt and vad_detector.is_speaking and not current_ptt:
                # VAD ‚Üí PTT –∏ –∫–Ω–æ–ø–∫–∞ –ù–ï –∑–∞–∂–∞—Ç–∞ ‚Üí —Å—Ä–∞–∑—É –∑–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ–∫—É—â—É—é —Ñ—Ä–∞–∑—É
                vad_detector.is_speaking = False
                phrase_completed = True
                print("‚è∏Ô∏è –ó–∞–≤–µ—Ä—à–∞—é —Ñ—Ä–∞–∑—É –∏–∑-–∑–∞ —Å–º–µ–Ω—ã —Ä–µ–∂–∏–º–∞ (–∫–Ω–æ–ø–∫–∞ –Ω–µ –∑–∞–∂–∞—Ç–∞)")
            vad_detector.last_use_ptt = current_use_ptt

        # 2. –õ–æ–≥–∏–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —Ä–µ–∂–∏–º–∞
        if current_use_ptt:  # Push-to-talk
            # Edge-detection –Ω–∞–∂–∞—Ç–∏—è/–æ—Ç–ø—É—Å–∫–∞–Ω–∏—è
            if current_ptt != vad_detector.last_ptt_state:
                if current_ptt:  # –Ω–∞–∂–∞–ª–∏
                    vad_detector.is_speaking = True
                    print("üé§ Push-to-talk –∑–∞–ø–∏—Å—å –Ω–∞—á–∞–ª–∞—Å—å")
                else:  # –æ—Ç–ø—É—Å—Ç–∏–ª–∏
                    if vad_detector.is_speaking:
                        vad_detector.is_speaking = False
                        phrase_completed = True
                        print("‚è∏Ô∏è Push-to-talk –∑–∞–ø–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                vad_detector.last_ptt_state = current_ptt

            # –ö–æ–ø–∏—Ä—É–µ–º –∞—É–¥–∏–æ —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞ –∫–Ω–æ–ø–∫–∞ –∑–∞–∂–∞—Ç–∞
            if current_ptt:
                audio_float = audio_chunk.astype(np.float32) / 32768.0
                vad_detector.audio_buffer.extend(audio_float)

        else:  # VAD-—Ä–µ–∂–∏–º
            phrase_completed = vad_detector.process_chunk(audio_chunk)

        # 3. –ï—Å–ª–∏ —Ñ—Ä–∞–∑–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å ‚Äî —Å—Ä–∞–∑—É –∏–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ –∏ –∫–ª–∞–¥—ë–º –≤ –æ—á–µ—Ä–µ–¥—å
        if phrase_completed:
            try:
                audio_to_yield = vad_detector.get_audio()
                if len(audio_to_yield) >= SAMPLE_RATE * MIN_SPEECH_DURATION:
                    audio_data_queue.put(audio_to_yield)          # ‚Üê —Ç–µ–ø–µ—Ä—å –∫–ª–∞–¥—ë–º —Å–∞–º–æ –∞—É–¥–∏–æ!
                else:
                    print("–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∞—É–¥–∏–æ –≤ –æ—á–µ—Ä–µ–¥—å: {e}")

    # –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ –∑–∞–ø–∏—Å–∏
    def run_recording():
        with sd.RawInputStream(
            samplerate=SAMPLE_RATE,
            blocksize=CHUNK_SIZE,
            dtype='int16',
            channels=1,
            callback=audio_callback
        ):
            while not stop_recording.is_set():
                sd.sleep(200)

    recording_thread = threading.Thread(target=run_recording, daemon=True)
    recording_thread.start()

    try:
        while not stop_event.is_set():
            await asyncio.sleep(0.1)
            try:
                # –¢–µ–ø–µ—Ä—å –≤ –æ—á–µ—Ä–µ–¥–∏ –ª–µ–∂–∏—Ç –≥–æ—Ç–æ–≤–æ–µ np.array –∞—É–¥–∏–æ
                audio_float = audio_data_queue.get_nowait()
                yield audio_float
            except Empty:
                continue
    finally:
        stop_recording.set()
        if recording_thread.is_alive():
            recording_thread.join(timeout=1)
        print("–ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

async def transcribe_audio_stream(transcriber):
    """–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏"""
    print("üéß –°–ª—É—à–∞—é... –ì–æ–≤–æ—Ä–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å")
   
    async for audio_data in record_audio_with_vad():
        if len(audio_data) == 0:
            continue
           
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –∞—É–¥–∏–æ: {len(audio_data)/SAMPLE_RATE:.2f} —Å–µ–∫")
       
        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å
        try:
            text, info = transcriber.transcribe(audio_data)
            if text and text.strip():
                print(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {text.strip()}")
                yield text.strip()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")

async def process_conversation():
    global conversation_history  # ‚Üê –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ, —á—Ç–æ–±—ã –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–∏–∞–ª–æ–≥–∞
    async for recognized_text in transcribe_audio_stream(transcriber):
        if not recognized_text:
            continue
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        conversation_history.append({"role": "user", "content": recognized_text})
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(conversation_history) > MAX_HISTORY_LENGTH:
            conversation_history = [conversation_history[0]] + conversation_history[-MAX_HISTORY_LENGTH + 1:]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º system prompt
        
        print("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç —Å LLM...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Qwen —Å attention_mask
        tokenized = tokenizer.apply_chat_template(
            conversation_history,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True
        ).to("cuda:0")
        
        # Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        generation_kwargs = {
            "input_ids": tokenized.input_ids,
            "attention_mask": tokenized.attention_mask,
            "streamer": streamer,
            "max_new_tokens": 200,
            "do_sample": True,
            "temperature": 0.7,
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º thread
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # –°–æ–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ —á–∞—Å—Ç—è–º –∏ —Å—Ç—Ä–∏–º –≤ TTS
        response_text = ""
        chunk = ""
        
        for new_token in streamer:
            if new_token:
                response_text += new_token
                chunk += new_token
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if chunk.endswith(('.', '!', '?')):
                    print(f"ü§ñ –ß–∞—Å—Ç—å –æ—Ç–≤–µ—Ç–∞: {chunk}")
                    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫ (TTS + play)
                    await generate_and_play_tts(chunk.strip())
                    chunk = ""
        
        # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–π —á–∞–Ω–∫ –≤ –∫–æ–Ω—Ü–µ
        if chunk.strip():
            print(f"ü§ñ –ü–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å: {chunk}")
            await generate_and_play_tts(chunk.strip())
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
        conversation_history.append({"role": "assistant", "content": response_text.strip()})
        
        # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è thread
        thread.join()

async def generate_and_play_tts(text_chunk):
    if not text_chunk:
        return
    
    print(f"üéµ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ—á—å –¥–ª—è: {text_chunk}")
    wavs, sr = tts_model.generate_voice_clone(
        text=text_chunk,
        language="Russian",
        voice_clone_prompt=prompt_items,
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    response_wav = Path("E:/Coding/talking-head-assistant/generated_speech_audio") / f"{person_name}_chunk_{stamp}.wav"
    sf.write(response_wav, wavs[0], sr)
    print(f"üíæ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {response_wav}")
    
    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥—É–±
    video_path = await run_wav2lip(response_wav, person_name)
    
    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (sequential, —Ç.–∫. await)
    if video_path:
        await play_video_with_audio(video_path, response_wav)

async def run_wav2lip(audio_path, person_name):
    """–ó–∞–ø—É—Å–∫ Wav2Lip –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    WAV2LIP_PYTHON = r"C:\Users\Shurik\anaconda3\envs\wav2lip\python.exe"
    WAV2LIP_DIR = Path(r"E:\Coding\talking-head-assistant\third_party\Wav2Lip")
    RESULT_VIDEO_DIR = Path(r"E:\Coding\talking-head-assistant\result_video")
   
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    result_video_path = RESULT_VIDEO_DIR / f"{person_name}_{stamp}.mp4"
   
    print("üëÑ –ó–∞–ø—É—Å–∫–∞—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≥—É–±...")
   
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å asyncio
    def run_command():
        wav2lip_cmd = [
            WAV2LIP_PYTHON,
            "inference.py",
            "--checkpoint_path", "checkpoints/wav2lip_gan.pth",
            "--face", "face_video.mp4",
            "--audio", str(audio_path),
            "--outfile", str(result_video_path),
            "--nosmooth"
        ]
       
        try:
            result = subprocess.run(
                wav2lip_cmd,
                cwd=WAV2LIP_DIR,
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            if result.returncode == 0:
                print("‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                return result_video_path
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ Wav2Lip: {result.stderr}")
                return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Wav2Lip: {e}")
            return None
   
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø—É–ª–µ –ø–æ—Ç–æ–∫–æ–≤
    loop = asyncio.get_event_loop()
    try:
        video_path = await loop.run_in_executor(None, run_command)
        return video_path
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ Wav2Lip: {e}")
        return None

async def play_video_with_audio(video_path, audio_path):
    """–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ"""
    if not video_path.exists():
        print("‚ùå –í–∏–¥–µ–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
   
    print(f"üé¨ –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ: {video_path.name}")
   
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ
    def play_audio():
        try:
            data, sr = sf.read(audio_path)
            sd.play(data, sr)
            sd.wait()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ: {e}")
   
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    audio_thread = threading.Thread(target=play_audio, daemon=True)
    audio_thread.start()
   
    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ OpenCV
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ")
        return
   
    fps = cap.get(cv2.CAP_PROP_FPS)
    delay = int(1000 / fps) if fps > 0 else 30
   
    print("‚ñ∂Ô∏è –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (–Ω–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞)...")
   
    while True:
        ret, frame = cap.read()
        if not ret:
            break
       
        cv2.imshow("Avatar", frame)
       
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∂–∞—Ç–∏–µ –∫–ª–∞–≤–∏—à–∏ 'q'
        if cv2.waitKey(delay) & 0xFF == ord('q'):
            break
       
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å –ª–∏ –∞—É–¥–∏–æ
        if not audio_thread.is_alive():
            break
   
    cap.release()
    cv2.destroyAllWindows()

def toggle_record_mode():
    global USE_PUSH_TO_TALK
    USE_PUSH_TO_TALK = not USE_PUSH_TO_TALK
    print("üéõ –†–µ–∂–∏–º –∑–∞–ø–∏—Å–∏:",
          "Push-to-talk" if USE_PUSH_TO_TALK else "VAD")

def start_keyboard_listener():
    def on_press(key):
        if key == PUSH_TO_TALK_KEY:
            push_to_talk_active.set()
        elif key == pynput_keyboard.Key.f8:
            toggle_record_mode()

    def on_release(key):
        if key == PUSH_TO_TALK_KEY:
            push_to_talk_active.clear()

    listener = pynput_keyboard.Listener(on_press=on_press, on_release=on_release)
    listener.start()  # –ó–∞–ø—É—Å–∫–∞–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    listener.wait()   # –ñ–¥—ë—Ç, –ø–æ–∫–∞ –Ω–µ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è (–Ω–æ –ø–æ—Å–∫–æ–ª—å–∫—É daemon, –æ–∫)

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 50)
    print("ü§ñ –ì–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ—á–∏")
    print("=" * 50)
    print("\n–ö–æ–º–∞–Ω–¥—ã:")
    print(" ‚Ä¢ –ù–∞—á–Ω–∏—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å - —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ä–µ—á—å")
    print(" ‚Ä¢ –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print()
   
    # ‚å®Ô∏è –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –≤ —Ñ–æ–Ω–µ
    keyboard_thread = threading.Thread(target=start_keyboard_listener, daemon=True)
    keyboard_thread.start()
   
    try:
        await process_conversation()
    except KeyboardInterrupt:
        print("\n\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        stop_event.set()
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        stop_event.set()

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    try:
        import webrtcvad
        print("‚úÖ VAD –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except ImportError:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ webrtcvad: pip install webrtcvad")
        exit(1)
   
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ: {e}")
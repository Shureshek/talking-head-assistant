import asyncio
import queue
from queue import Queue, Empty
import threading
import sounddevice as sd
import numpy as np
import soundfile as sf
import subprocess
from pathlib import Path
import cv2
from datetime import datetime
import torch
import webrtcvad
from collections import deque

from src.asr.whisper_asr import WhisperTranscriber
from src.voice_clone.clone_manager import VoiceCloneManager
from qwen_tts import Qwen3TTSModel

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
SAMPLE_RATE = 16000
CHUNK_SIZE = 480  # 30ms –¥–ª—è VAD (16000 * 0.03)
VAD_AGGRESSIVENESS = 2  # 0-3, –≥–¥–µ 3 —Å–∞–º—ã–π –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π
SILENCE_TIMEOUT = 1.5  # —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ—Ä–∞–∑—ã
MIN_SPEECH_DURATION = 0.5  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

audio_data_queue = Queue()          # thread-safe –æ—á–µ—Ä–µ–¥—å –∏–∑ stdlib
stop_event = threading.Event()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TTS –º–æ–¥–µ–ª–∏
tts_model = Qwen3TTSModel.from_pretrained(
    "Qwen/Qwen3-TTS-12Hz-0.6B-Base",
    device_map="cuda:0",
    dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)

class VoiceActivityDetector:
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)
        self.chunk_size = int(sample_rate * 0.03)  # 30ms
        self.silence_frames_threshold = int(SILENCE_TIMEOUT / 0.03)  # 1.5 —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã
        self.silence_counter = 0
        self.is_speaking = False
        self.audio_buffer = []
        
    def process_chunk(self, audio_chunk):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ-—á–∞–Ω–∫–∞ —á–µ—Ä–µ–∑ VAD"""
        try:
            # audio_chunk —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å int16
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            if len(audio_chunk) != self.chunk_size:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if len(audio_chunk) < self.chunk_size:
                    # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                    padding = np.zeros(self.chunk_size - len(audio_chunk), dtype=np.int16)
                    audio_chunk = np.concatenate([audio_chunk, padding])
                else:
                    audio_chunk = audio_chunk[:self.chunk_size]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ bytes –¥–ª—è VAD
            audio_bytes = audio_chunk.tobytes()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —á–∞–Ω–∫ —Ä–µ—á—å
            is_speech = self.vad.is_speech(audio_bytes, self.sample_rate)
            
            if is_speech:
                self.silence_counter = 0
                if not self.is_speaking:
                    self.is_speaking = True
                    print("üé§ –†–µ—á—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞—é –∑–∞–ø–∏—Å—å...")
            else:
                if self.is_speaking:
                    self.silence_counter += 1
                    if self.silence_counter >= self.silence_frames_threshold:
                        self.is_speaking = False
                        print("‚è∏Ô∏è  –†–µ—á—å –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...")
                        return True  # –°–∏–≥–Ω–∞–ª –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ñ—Ä–∞–∑—ã
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –≤ –±—É—Ñ–µ—Ä (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32 –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
            audio_float = audio_chunk.astype(np.float32) / 32768.0
            self.audio_buffer.extend(audio_float)
            return False
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ VAD: {e}")
            return False
    
    def get_audio(self):
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ –∏ –æ—á–∏—Å—Ç–∏—Ç—å –±—É—Ñ–µ—Ä"""
        if len(self.audio_buffer) == 0:
            return np.array([], dtype=np.float32)
        
        audio = np.array(self.audio_buffer, dtype=np.float32)
        self.audio_buffer = []
        self.silence_counter = 0
        self.is_speaking = False
        return audio

async def record_audio_with_vad():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ —Å VAD (—É–ø—Ä–æ—â—ë–Ω–Ω–æ, —á–µ—Ä–µ–∑ –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—É—é –æ—á–µ—Ä–µ–¥—å)."""
    vad_detector = VoiceActivityDetector()

    def audio_callback(indata, frames, time, status):
        """Callback –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ø–æ—Ç–æ–∫–µ –∑–≤—É–∫–æ–≤–æ–≥–æ –±—ç–∫—ç–Ω–¥–∞)."""
        if status:
            print(f"–ê—É–¥–∏–æ —Å—Ç–∞—Ç—É—Å: {status}")

        # indata –º–æ–∂–µ—Ç –±—ã—Ç—å bytes / memoryview –≤ RawInputStream, –∏–ª–∏ numpy array –≤ InputStream.
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ int16 numpy array.
        try:
            # –ï—Å–ª–∏ indata —É–∂–µ ndarray —Å dtype=int16 ‚Äî —ç—Ç–æ –æ–∫.
            if isinstance(indata, np.ndarray):
                audio_chunk = indata.copy().astype(np.int16).flatten()
            else:
                # RawInputStream: indata ‚Äî bytes/bytearray/memoryview
                audio_chunk = np.frombuffer(indata, dtype=np.int16).copy()
        except Exception as e:
            print("–û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ callback:", e)
            return

        phrase_completed = vad_detector.process_chunk(audio_chunk)

        # –ö–ª–∞–¥—ë–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—É—é –æ—á–µ—Ä–µ–¥—å ‚Äî –±–µ–∑ asyncio
        try:
            audio_data_queue.put_nowait(("AUDIO", audio_chunk))
        except Exception as e:
            # –≤ –ø–µ—Ä–µ—Å—ã–ª–∫–µ –æ–±—ã—á–Ω–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–π, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π:
            print("–û—à–∏–±–∫–∞ put –≤ –æ—á–µ—Ä–µ–¥—å:", e)

        if phrase_completed:
            # —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª –æ –∫–æ–Ω—Ü–µ —Ñ—Ä–∞–∑—ã
            try:
                audio_data_queue.put_nowait(("PHRASE_END", None))
            except Exception as e:
                print("–û—à–∏–±–∫–∞ put PHRASE_END:", e)

    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏ (sounddevice) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º InputStream –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞,
    # –Ω–æ RawInputStream —Ç–æ–∂–µ –≤–æ–∑–º–æ–∂–µ–Ω. InputStream –æ—Ç–¥–∞—ë—Ç ndarray float32 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
    def run_recording():
        with sd.RawInputStream(
            samplerate=SAMPLE_RATE,
            blocksize=CHUNK_SIZE,
            dtype='int16',
            channels=1,
            callback=audio_callback
        ):
            while not stop_event.is_set():
                sd.sleep(200)

    recording_thread = threading.Thread(target=run_recording, daemon=True)
    recording_thread.start()

    loop = asyncio.get_running_loop()

    try:
        while not stop_event.is_set():
            try:
                # –î–æ—Å—Ç–∞—ë–º –∏–∑ –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ—á–µ—Ä–µ–¥–∏, —Å –Ω–µ–±–æ–ª—å—à–∏–º —Ç–∞–π–º–∞—É—Ç–æ–º
                event_type, payload = await loop.run_in_executor(
                    None, audio_data_queue.get, True, 0.15
                )

                if event_type == "PHRASE_END":
                    audio_float = vad_detector.get_audio()
                    if len(audio_float) >= SAMPLE_RATE * MIN_SPEECH_DURATION:
                        yield audio_float
                    else:
                        print("–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º")

                else:
                    # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —á–∞–Ω–∫–∏ (–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º)
                    # –∑–¥–µ—Å—å –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º ‚Äî VAD –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç –≤ vad_detector.audio_buffer
                    pass

            except Exception as e:
                # –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –±—ã–ª–∞ –ø—É—Å—Ç–∞ ‚Äî get —Å —Ç–∞–π–º–∞—É—Ç–æ–º –≤—ã–±—Ä–æ—Å–∏—Ç Empty,
                # run_in_executor –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –µ–≥–æ –≤ Exception, –ª–æ–≤–∏–º –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                if isinstance(e, Empty) or "Empty" in str(e):
                    continue
                print("–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—á–µ—Ä–µ–¥–∏:", e)
                break

    finally:
        stop_event.set()
        if recording_thread.is_alive():
            recording_thread.join(timeout=1)

async def transcribe_audio_stream(transcriber):
    """–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏"""
    print("üéß –°–ª—É—à–∞—é... –ì–æ–≤–æ—Ä–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å")
    
    async for audio_data in record_audio_with_vad():
        if len(audio_data) == 0:
            continue
            
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –∞—É–¥–∏–æ: {len(audio_data)/SAMPLE_RATE:.2f} —Å–µ–∫")
        
        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å
        try:
            text, info = transcriber.transcribe(audio_data)
            if text and text.strip():
                print(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {text.strip()}")
                yield text.strip()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")

async def process_conversation():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞"""
    transcriber = WhisperTranscriber()
    manager = VoiceCloneManager(model=tts_model)
    person_name = "Julia"
    
    print("üé≠ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
    prompt_items = manager.load_or_create_clone(person_name)
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–∏–∞–ª–æ–≥–∞
    async for recognized_text in transcribe_audio_stream(transcriber):
        if not recognized_text:
            continue
            
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response_text = f"–í—ã —Å–∫–∞–∑–∞–ª–∏: {recognized_text}"
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response_text}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ
        print("üéµ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ—á—å...")
        wavs, sr = tts_model.generate_voice_clone(
            text=response_text,
            language="Russian",
            voice_clone_prompt=prompt_items,
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ
        response_wav = Path("E:/Coding/talking-head-assistant/generated_speech_audio") / f"{person_name}_response.wav"
        sf.write(response_wav, wavs[0], sr)
        print(f"üíæ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {response_wav}")
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥—É–±
        await run_wav2lip(response_wav, person_name)
        
        # –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        await play_video_with_audio(response_wav)

async def run_wav2lip(audio_path, person_name):
    """–ó–∞–ø—É—Å–∫ Wav2Lip –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    WAV2LIP_PYTHON = r"C:\Users\Shurik\anaconda3\envs\wav2lip\python.exe"
    WAV2LIP_DIR = Path(r"E:\Coding\talking-head-assistant\third_party\Wav2Lip")
    RESULT_VIDEO_DIR = Path(r"E:\Coding\talking-head-assistant\result_video")
    
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_video_path = RESULT_VIDEO_DIR / f"{person_name}_{stamp}.mp4"
    
    print("üëÑ –ó–∞–ø—É—Å–∫–∞—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≥—É–±...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å asyncio
    def run_command():
        wav2lip_cmd = [
            WAV2LIP_PYTHON,
            "inference.py",
            "--checkpoint_path", "checkpoints/wav2lip_gan.pth",
            "--face", "face_video.mp4",
            "--audio", str(audio_path),
            "--outfile", str(result_video_path),
            "--nosmooth"
        ]
        
        try:
            result = subprocess.run(
                wav2lip_cmd,
                cwd=WAV2LIP_DIR,
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            if result.returncode == 0:
                print("‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                return result_video_path
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ Wav2Lip: {result.stderr}")
                return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Wav2Lip: {e}")
            return None
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø—É–ª–µ –ø–æ—Ç–æ–∫–æ–≤
    loop = asyncio.get_event_loop()
    try:
        video_path = await loop.run_in_executor(None, run_command)
        return video_path
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ Wav2Lip: {e}")
        return None

async def play_video_with_audio(audio_path):
    """–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ"""
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–∑–¥–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ
    video_dir = Path(r"E:\Coding\talking-head-assistant\result_video")
    video_files = list(video_dir.glob("*.mp4"))
    if not video_files:
        print("‚ùå –í–∏–¥–µ–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    latest_video = max(video_files, key=lambda x: x.stat().st_mtime)
    
    print(f"üé¨ –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ: {latest_video.name}")
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ
    def play_audio():
        try:
            data, sr = sf.read(audio_path)
            sd.play(data, sr)
            sd.wait()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ: {e}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    audio_thread = threading.Thread(target=play_audio, daemon=True)
    audio_thread.start()
    
    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ OpenCV
    cap = cv2.VideoCapture(str(latest_video))
    if not cap.isOpened():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ")
        return
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    delay = int(1000 / fps) if fps > 0 else 30
    
    print("‚ñ∂Ô∏è  –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (–Ω–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞)...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        cv2.imshow("Avatar", frame)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∂–∞—Ç–∏–µ –∫–ª–∞–≤–∏—à–∏ 'q'
        if cv2.waitKey(delay) & 0xFF == ord('q'):
            break
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å –ª–∏ –∞—É–¥–∏–æ
        if not audio_thread.is_alive():
            break
    
    cap.release()
    cv2.destroyAllWindows()

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 50)
    print("ü§ñ –ì–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ—á–∏")
    print("=" * 50)
    print("\n–ö–æ–º–∞–Ω–¥—ã:")
    print("  ‚Ä¢ –ù–∞—á–Ω–∏—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å - —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ä–µ—á—å")
    print("  ‚Ä¢ –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print()
    
    try:
        await process_conversation()
    except KeyboardInterrupt:
        print("\n\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    try:
        import webrtcvad
        print("‚úÖ VAD –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except ImportError:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ webrtcvad: pip install webrtcvad")
        exit(1)
    
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ: {e}")
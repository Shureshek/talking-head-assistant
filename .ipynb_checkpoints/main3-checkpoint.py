import asyncio
import queue
from queue import Queue, Empty
import threading
import sounddevice as sd
import numpy as np
import soundfile as sf
import subprocess
from pathlib import Path
import cv2
from datetime import datetime
import torch
import webrtcvad
from collections import deque
import keyboard

from src.asr.whisper_asr import WhisperTranscriber
from src.voice_clone.clone_manager import VoiceCloneManager
from qwen_tts import Qwen3TTSModel

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
SAMPLE_RATE = 16000
CHUNK_SIZE = 480  # 30ms –¥–ª—è VAD (16000 * 0.03)
VAD_AGGRESSIVENESS = 2  # 0-3, –≥–¥–µ 3 —Å–∞–º—ã–π –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π
SILENCE_TIMEOUT = 1.5  # —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ—Ä–∞–∑—ã
MIN_SPEECH_DURATION = 0.5  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

USE_PUSH_TO_TALK = False
PUSH_TO_TALK_KEY = 'space' # –∫–ª–∞–≤–∏—à–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
push_to_talk_active = threading.Event()

audio_data_queue = Queue()          # thread-safe –æ—á–µ—Ä–µ–¥—å –∏–∑ stdlib
stop_event = threading.Event()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TTS –º–æ–¥–µ–ª–∏
tts_model = Qwen3TTSModel.from_pretrained(
    "Qwen/Qwen3-TTS-12Hz-0.6B-Base",
    device_map="cuda:0",
    dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)

class VoiceActivityDetector:
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)
        self.chunk_size = int(sample_rate * 0.03)  # 30ms
        self.silence_frames_threshold = int(SILENCE_TIMEOUT / 0.03)  # 1.5 —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã
        self.silence_counter = 0
        self.is_speaking = False
        self.audio_buffer = []
        
    def process_chunk(self, audio_chunk):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ-—á–∞–Ω–∫–∞ —á–µ—Ä–µ–∑ VAD"""
        try:
            # audio_chunk —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å int16
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            if len(audio_chunk) != self.chunk_size:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if len(audio_chunk) < self.chunk_size:
                    # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                    padding = np.zeros(self.chunk_size - len(audio_chunk), dtype=np.int16)
                    audio_chunk = np.concatenate([audio_chunk, padding])
                else:
                    audio_chunk = audio_chunk[:self.chunk_size]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ bytes –¥–ª—è VAD
            audio_bytes = audio_chunk.tobytes()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —á–∞–Ω–∫ —Ä–µ—á—å
            is_speech = self.vad.is_speech(audio_bytes, self.sample_rate)
            
            if is_speech:
                self.silence_counter = 0
                if not self.is_speaking:
                    self.is_speaking = True
                    print("üé§ –†–µ—á—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞—é –∑–∞–ø–∏—Å—å...")
            else:
                if self.is_speaking:
                    self.silence_counter += 1
                    if self.silence_counter >= self.silence_frames_threshold:
                        self.is_speaking = False
                        print("‚è∏Ô∏è  –†–µ—á—å –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...")
                        return True  # –°–∏–≥–Ω–∞–ª –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ñ—Ä–∞–∑—ã
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –≤ –±—É—Ñ–µ—Ä (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32 –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
            audio_float = audio_chunk.astype(np.float32) / 32768.0
            self.audio_buffer.extend(audio_float)
            return False
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ VAD: {e}")
            return False
    
    def get_audio(self):
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ –∏ –æ—á–∏—Å—Ç–∏—Ç—å –±—É—Ñ–µ—Ä"""
        if len(self.audio_buffer) == 0:
            return np.array([], dtype=np.float32)
        
        audio = np.array(self.audio_buffer, dtype=np.float32)
        self.audio_buffer = []
        self.silence_counter = 0
        self.is_speaking = False
        return audio

async def record_audio_with_vad():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ —Å VAD"""
    vad_detector = VoiceActivityDetector()
    stop_recording = threading.Event()
    
    def audio_callback(indata, frames, time, status):
        """Callback –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ"""
        if status:
            print(f"–ê—É–¥–∏–æ —Å—Ç–∞—Ç—É—Å: {status}")
        
        # indata - —ç—Ç–æ –±—É—Ñ–µ—Ä –±–∞–π—Ç–æ–≤, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy int16
        # frames —É–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ (—Å—ç–º–ø–ª–æ–≤)
        audio_chunk = np.frombuffer(indata, dtype=np.int16)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ VAD
        phrase_completed = False

        if USE_PUSH_TO_TALK:
            # –∑–∞–ø–∏—Å—å –∏–¥—ë—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∞–∂–∞—Ç–∞ –∫–Ω–æ–ø–∫–∞
            if push_to_talk_active.is_set():
        
                if not vad_detector.is_speaking:
                    vad_detector.is_speaking = True
                    print("üé§ Push-to-talk –∑–∞–ø–∏—Å—å –Ω–∞—á–∞–ª–∞—Å—å")
        
                # –≤—Ä—É—á–Ω—É—é –∫–æ–ø–∏–º –∞—É–¥–∏–æ
                audio_float = audio_chunk.astype(np.float32) / 32768.0
                vad_detector.audio_buffer.extend(audio_float)
        
            else:
                # –∫–Ω–æ–ø–∫–∞ –æ—Ç–ø—É—â–µ–Ω–∞ ‚Üí –∑–∞–≤–µ—Ä—à–∞–µ–º —Ñ—Ä–∞–∑—É
                if vad_detector.is_speaking:
                    vad_detector.is_speaking = False
                    phrase_completed = True
                    print("‚è∏Ô∏è Push-to-talk –∑–∞–ø–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        else:
            # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π VAD —Ä–µ–∂–∏–º
            phrase_completed = vad_detector.process_chunk(audio_chunk)
        
        # –ï—Å–ª–∏ —Ñ—Ä–∞–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–± —ç—Ç–æ–º —á–µ—Ä–µ–∑ –æ—á–µ—Ä–µ–¥—å
        if phrase_completed:
            try:
                audio_data_queue.put("PHRASE_END")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –≤ –æ—á–µ—Ä–µ–¥—å: {e}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏
    def run_recording():
        with sd.RawInputStream(
            samplerate=SAMPLE_RATE,
            blocksize=CHUNK_SIZE,
            dtype='int16',
            channels=1,
            callback=audio_callback
        ):
            while not stop_recording.is_set():
                sd.sleep(200)

    recording_thread = threading.Thread(target=run_recording, daemon=True)
    recording_thread.start()

    try:
        while not stop_event.is_set():
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º asyncio.sleep –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è
                await asyncio.sleep(0.1)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ñ—Ä–∞–∑—ã
                try:
                    item = audio_data_queue.get_nowait()
                    if item == "PHRASE_END":
                        audio_float = vad_detector.get_audio()
                        if len(audio_float) >= SAMPLE_RATE * MIN_SPEECH_DURATION:
                            yield audio_float
                        else:
                            print("–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º")
                except Empty:
                    continue
                    
            except Exception as e:
                print("–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—á–µ—Ä–µ–¥–∏:", e)
                break
                
    finally:
        stop_recording.set()
        if recording_thread.is_alive():
            recording_thread.join(timeout=1)
        print("–ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

async def transcribe_audio_stream(transcriber):
    """–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏"""
    print("üéß –°–ª—É—à–∞—é... –ì–æ–≤–æ—Ä–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å")
    
    async for audio_data in record_audio_with_vad():
        if len(audio_data) == 0:
            continue
            
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –∞—É–¥–∏–æ: {len(audio_data)/SAMPLE_RATE:.2f} —Å–µ–∫")
        
        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å
        try:
            text, info = transcriber.transcribe(audio_data)
            if text and text.strip():
                print(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {text.strip()}")
                yield text.strip()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")

async def process_conversation():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞"""
    transcriber = WhisperTranscriber()
    manager = VoiceCloneManager(model=tts_model)
    person_name = "Julia"
    
    print("üé≠ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
    prompt_items = manager.load_or_create_clone(person_name)
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–∏–∞–ª–æ–≥–∞
    async for recognized_text in transcribe_audio_stream(transcriber):
        if not recognized_text:
            continue
            
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response_text = f"–í—ã —Å–∫–∞–∑–∞–ª–∏: {recognized_text}"
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response_text}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ
        print("üéµ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ—á—å...")
        wavs, sr = tts_model.generate_voice_clone(
            text=response_text,
            language="Russian",
            voice_clone_prompt=prompt_items,
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ
        response_wav = Path("E:/Coding/talking-head-assistant/generated_speech_audio") / f"{person_name}_response.wav"
        sf.write(response_wav, wavs[0], sr)
        print(f"üíæ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {response_wav}")
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥—É–±
        await run_wav2lip(response_wav, person_name)
        
        # –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        await play_video_with_audio(response_wav)

async def run_wav2lip(audio_path, person_name):
    """–ó–∞–ø—É—Å–∫ Wav2Lip –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    WAV2LIP_PYTHON = r"C:\Users\Shurik\anaconda3\envs\wav2lip\python.exe"
    WAV2LIP_DIR = Path(r"E:\Coding\talking-head-assistant\third_party\Wav2Lip")
    RESULT_VIDEO_DIR = Path(r"E:\Coding\talking-head-assistant\result_video")
    
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_video_path = RESULT_VIDEO_DIR / f"{person_name}_{stamp}.mp4"
    
    print("üëÑ –ó–∞–ø—É—Å–∫–∞—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≥—É–±...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å asyncio
    def run_command():
        wav2lip_cmd = [
            WAV2LIP_PYTHON,
            "inference.py",
            "--checkpoint_path", "checkpoints/wav2lip_gan.pth",
            "--face", "face_video.mp4",
            "--audio", str(audio_path),
            "--outfile", str(result_video_path),
            "--nosmooth"
        ]
        
        try:
            result = subprocess.run(
                wav2lip_cmd,
                cwd=WAV2LIP_DIR,
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            if result.returncode == 0:
                print("‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                return result_video_path
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ Wav2Lip: {result.stderr}")
                return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Wav2Lip: {e}")
            return None
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø—É–ª–µ –ø–æ—Ç–æ–∫–æ–≤
    loop = asyncio.get_event_loop()
    try:
        video_path = await loop.run_in_executor(None, run_command)
        return video_path
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ Wav2Lip: {e}")
        return None

async def play_video_with_audio(audio_path):
    """–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ"""
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–∑–¥–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ
    video_dir = Path(r"E:\Coding\talking-head-assistant\result_video")
    video_files = list(video_dir.glob("*.mp4"))
    if not video_files:
        print("‚ùå –í–∏–¥–µ–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    latest_video = max(video_files, key=lambda x: x.stat().st_mtime)
    
    print(f"üé¨ –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ: {latest_video.name}")
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ
    def play_audio():
        try:
            data, sr = sf.read(audio_path)
            sd.play(data, sr)
            sd.wait()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ: {e}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    audio_thread = threading.Thread(target=play_audio, daemon=True)
    audio_thread.start()
    
    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ OpenCV
    cap = cv2.VideoCapture(str(latest_video))
    if not cap.isOpened():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ")
        return
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    delay = int(1000 / fps) if fps > 0 else 30
    
    print("‚ñ∂Ô∏è  –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (–Ω–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞)...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        cv2.imshow("Avatar", frame)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∂–∞—Ç–∏–µ –∫–ª–∞–≤–∏—à–∏ 'q'
        if cv2.waitKey(delay) & 0xFF == ord('q'):
            break
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å –ª–∏ –∞—É–¥–∏–æ
        if not audio_thread.is_alive():
            break
    
    cap.release()
    cv2.destroyAllWindows()

def start_keyboard_listener():
    def on_press(event):
        if event.name == PUSH_TO_TALK_KEY:
            push_to_talk_active.set()

    def on_release(event):
        if event.name == PUSH_TO_TALK_KEY:
            push_to_talk_active.clear()

    keyboard.on_press(on_press)
    keyboard.on_release(on_release)

    keyboard.add_hotkey('f8', toggle_record_mode)

def toggle_record_mode():
    global USE_PUSH_TO_TALK
    USE_PUSH_TO_TALK = not USE_PUSH_TO_TALK
    print("üéõ –†–µ–∂–∏–º –∑–∞–ø–∏—Å–∏:", 
          "Push-to-talk" if USE_PUSH_TO_TALK else "VAD")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 50)
    print("ü§ñ –ì–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ—á–∏")
    print("=" * 50)
    print("\n–ö–æ–º–∞–Ω–¥—ã:")
    print("  ‚Ä¢ –ù–∞—á–Ω–∏—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å - —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ä–µ—á—å")
    print("  ‚Ä¢ –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print()
    
    # ‚å®Ô∏è –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã
    start_keyboard_listener()
    
    try:
        await process_conversation()
    except KeyboardInterrupt:
        print("\n\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        stop_event.set()
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        stop_event.set()

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    try:
        import webrtcvad
        print("‚úÖ VAD –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except ImportError:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ webrtcvad: pip install webrtcvad")
        exit(1)
    
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ: {e}")
import asyncio
import queue
import threading
import sounddevice as sd
import numpy as np
import soundfile as sf
import subprocess
from pathlib import Path
import cv2
from datetime import datetime
import torch
import webrtcvad  # –î–ª—è VAD
from collections import deque

from src.asr.whisper_asr import WhisperTranscriber
from src.voice_clone.clone_manager import VoiceCloneManager
from qwen_tts import Qwen3TTSModel

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
SAMPLE_RATE = 16000
CHUNK_SIZE = 480  # 30ms –¥–ª—è VAD (16000 * 0.03)
VAD_AGGRESSIVENESS = 2  # 0-3, –≥–¥–µ 3 —Å–∞–º—ã–π –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π
SILENCE_TIMEOUT = 1.5  # —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ—Ä–∞–∑—ã
MIN_SPEECH_DURATION = 0.5  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
audio_buffer = deque(maxlen=int(SAMPLE_RATE * 10))  # –±—É—Ñ–µ—Ä –Ω–∞ 10 —Å–µ–∫—É–Ω–¥
is_recording = False
current_phrase = []
vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TTS –º–æ–¥–µ–ª–∏
tts_model = Qwen3TTSModel.from_pretrained(
    "Qwen/Qwen3-TTS-12Hz-0.6B-Base",
    device_map="cuda:0",
    dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)

class VoiceActivityDetector:
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.vad = webrtcvad.Vad(2)
        self.chunk_size = int(sample_rate * 0.03)  # 30ms
        self.silence_frames_threshold = int(1.5 / 0.03)  # 1.5 —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã
        self.silence_counter = 0
        self.is_speaking = False
        self.audio_buffer = []
        
    def process_chunk(self, audio_chunk):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ-—á–∞–Ω–∫–∞ —á–µ—Ä–µ–∑ VAD"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ 16-bit PCM
            audio_int16 = (audio_chunk * 32767).astype(np.int16)
            audio_bytes = audio_int16.tobytes()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —á–∞–Ω–∫ —Ä–µ—á—å
            is_speech = self.vad.is_speech(audio_bytes, self.sample_rate)
            
            if is_speech:
                self.silence_counter = 0
                if not self.is_speaking:
                    self.is_speaking = True
                    print("üé§ –†–µ—á—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞—é –∑–∞–ø–∏—Å—å...")
            else:
                if self.is_speaking:
                    self.silence_counter += 1
                    if self.silence_counter >= self.silence_frames_threshold:
                        self.is_speaking = False
                        print("‚è∏Ô∏è  –†–µ—á—å –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...")
                        return True  # –°–∏–≥–Ω–∞–ª –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ñ—Ä–∞–∑—ã
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –≤ –±—É—Ñ–µ—Ä
            self.audio_buffer.extend(audio_chunk)
            return False
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ VAD: {e}")
            return False
    
    def get_audio(self):
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ –∏ –æ—á–∏—Å—Ç–∏—Ç—å –±—É—Ñ–µ—Ä"""
        audio = np.array(self.audio_buffer, dtype=np.float32)
        self.audio_buffer = []
        self.silence_counter = 0
        return audio

async def record_audio_with_vad():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ —Å VAD"""
    vad_detector = VoiceActivityDetector()
    audio_queue = asyncio.Queue()
    stop_event = asyncio.Event()
    
    def audio_callback(indata, frames, time, status):
        """Callback –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ"""
        if status:
            print(f"–ê—É–¥–∏–æ —Å—Ç–∞—Ç—É—Å: {status}")
        
        # indata - —ç—Ç–æ –±—É—Ñ–µ—Ä –±–∞–π—Ç–æ–≤, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy int16
        # frames —É–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ (—Å—ç–º–ø–ª–æ–≤)
        audio_chunk = np.frombuffer(indata, dtype=np.int16)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ VAD
        phrase_completed = vad_detector.process_chunk(audio_chunk)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        try:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –æ—á–µ—Ä–µ–¥—å
            asyncio.run_coroutine_threadsafe(
                audio_queue.put((audio_chunk.copy(), phrase_completed)), 
                asyncio.get_event_loop()
            )
        except RuntimeError:
            # –ï—Å–ª–∏ —Ü–∏–∫–ª —Å–æ–±—ã—Ç–∏–π –Ω–µ –∑–∞–ø—É—â–µ–Ω, –∑–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—ã–π –ø–æ—Ç–æ–∫ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            pass
        
        # –ï—Å–ª–∏ —Ñ—Ä–∞–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–± —ç—Ç–æ–º
        if phrase_completed:
            try:
                asyncio.run_coroutine_threadsafe(
                    audio_queue.put(("PHRASE_END", None)), 
                    asyncio.get_event_loop()
                )
            except RuntimeError:
                pass
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏
    def run_recording():
        with sd.RawInputStream(
            samplerate=SAMPLE_RATE,
            blocksize=CHUNK_SIZE,
            dtype='int16',
            channels=1,
            callback=audio_callback
        ):
            while not stop_event.is_set():
                sd.sleep(100)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    recording_thread = threading.Thread(target=run_recording)
    recording_thread.start()
    
    try:
        while True:
            # –ñ–¥–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
            try:
                data = await asyncio.wait_for(audio_queue.get(), timeout=0.1)
                
                if data == ("PHRASE_END", None):
                    # –ü–æ–ª—É—á–∞–µ–º —Å–æ–±—Ä–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ
                    audio_data = vad_detector.get_audio()
                    if len(audio_data) > SAMPLE_RATE * MIN_SPEECH_DURATION:
                        yield audio_data
                    else:
                        print("‚ö†Ô∏è  –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Ñ—Ä–∞–∑–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—é")
                        
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –≤ –æ—á–µ—Ä–µ–¥–∏: {e}")
                break
                
    finally:
        stop_event.set()
        recording_thread.join()

async def transcribe_audio_stream(transcriber):
    """–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏"""
    print("üéß –°–ª—É—à–∞—é... –ì–æ–≤–æ—Ä–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å")
    
    async for audio_data in record_audio_with_vad():
        if len(audio_data) == 0:
            continue
            
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –∞—É–¥–∏–æ: {len(audio_data)/SAMPLE_RATE:.2f} —Å–µ–∫")
        
        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å
        try:
            text, info = transcriber.transcribe(audio_data)
            if text and text.strip():
                print(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {text.strip()}")
                yield text.strip()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")

async def process_conversation():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞"""
    transcriber = WhisperTranscriber()
    manager = VoiceCloneManager(model=tts_model)
    person_name = "Julia"
    
    print("üé≠ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
    prompt_items = manager.load_or_create_clone(person_name)
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–∏–∞–ª–æ–≥–∞
    async for recognized_text in transcribe_audio_stream(transcriber):
        if not recognized_text:
            continue
            
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response_text = f"–í—ã —Å–∫–∞–∑–∞–ª–∏: {recognized_text}"
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {response_text}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ
        print("üéµ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ—á—å...")
        wavs, sr = tts_model.generate_voice_clone(
            text=response_text,
            language="Russian",
            voice_clone_prompt=prompt_items,
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ
        response_wav = Path("E:/Coding/talking-head-assistant/generated_speech_audio") / f"{person_name}_response.wav"
        sf.write(response_wav, wavs[0], sr)
        print(f"üíæ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {response_wav}")
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥—É–±
        await run_wav2lip(response_wav, person_name)
        
        # –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        await play_video_with_audio(response_wav)

async def run_wav2lip(audio_path, person_name):
    """–ó–∞–ø—É—Å–∫ Wav2Lip –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    WAV2LIP_PYTHON = r"C:\Users\Shurik\anaconda3\envs\wav2lip\python.exe"
    WAV2LIP_DIR = Path(r"E:\Coding\talking-head-assistant\third_party\Wav2Lip")
    RESULT_VIDEO_DIR = Path(r"E:\Coding\talking-head-assistant\result_video")
    
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_video_path = RESULT_VIDEO_DIR / f"{person_name}_{stamp}.mp4"
    
    print("üëÑ –ó–∞–ø—É—Å–∫–∞—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≥—É–±...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å asyncio
    def run_command():
        wav2lip_cmd = [
            WAV2LIP_PYTHON,
            "inference.py",
            "--checkpoint_path", "checkpoints/wav2lip_gan.pth",
            "--face", "face_video.mp4",
            "--audio", str(audio_path),
            "--outfile", str(result_video_path),
            "--nosmooth"
        ]
        
        try:
            result = subprocess.run(
                wav2lip_cmd,
                cwd=WAV2LIP_DIR,
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                print("‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                return result_video_path
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ Wav2Lip: {result.stderr}")
                return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Wav2Lip: {e}")
            return None
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –ø—É–ª–µ –ø–æ—Ç–æ–∫–æ–≤
    loop = asyncio.get_event_loop()
    video_path = await loop.run_in_executor(None, run_command)
    return video_path

async def play_video_with_audio(audio_path):
    """–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ"""
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–∑–¥–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ
    video_dir = Path(r"E:\Coding\talking-head-assistant\result_video")
    video_files = list(video_dir.glob("*.mp4"))
    if not video_files:
        print("‚ùå –í–∏–¥–µ–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    latest_video = max(video_files, key=lambda x: x.stat().st_mtime)
    
    print(f"üé¨ –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ: {latest_video.name}")
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ
    def play_audio():
        try:
            data, sr = sf.read(audio_path)
            sd.play(data, sr)
            sd.wait()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ: {e}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    audio_thread = threading.Thread(target=play_audio, daemon=True)
    audio_thread.start()
    
    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ OpenCV
    cap = cv2.VideoCapture(str(latest_video))
    if not cap.isOpened():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ")
        return
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    delay = int(1000 / fps) if fps > 0 else 30
    
    print("‚ñ∂Ô∏è  –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (–Ω–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞)...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        cv2.imshow("Avatar", frame)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∂–∞—Ç–∏–µ –∫–ª–∞–≤–∏—à–∏ 'q'
        if cv2.waitKey(delay) & 0xFF == ord('q'):
            break
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å –ª–∏ –∞—É–¥–∏–æ
        if not audio_thread.is_alive():
            break
    
    cap.release()
    cv2.destroyAllWindows()

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 50)
    print("ü§ñ –ì–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ—á–∏")
    print("=" * 50)
    print("\n–ö–æ–º–∞–Ω–¥—ã:")
    print("  ‚Ä¢ –ù–∞—á–Ω–∏—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å - —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ä–µ—á—å")
    print("  ‚Ä¢ –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    print()
    
    try:
        await process_conversation()
    except KeyboardInterrupt:
        print("\n\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    try:
        import webrtcvad
        print("‚úÖ VAD –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except ImportError:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ webrtcvad: pip install webrtcvad")
        exit(1)
    
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    asyncio.run(main())